# Supabase Configuration
SUPABASE_URL=your_supabase_url_here
SUPABASE_KEY=your_supabase_anon_key_here

# AI Service API Keys
GEMINI_API_KEY=your_gemini_api_key_here
OPENAI_API_KEY=your_openai_api_key_here

# Per-Task LLM Configuration (format: provider:model)
# Available providers: gemini, openai
# Gemini models: gemini-2.5-flash (cost-effective, supports temperature control)
#                gemini-2.5-pro (higher quality, more expensive)
# OpenAI models: gpt-4o, gpt-4-turbo (support temperature)
# Note: GPT-5/o1 reasoning models do NOT support temperature parameter

# Using Gemini 2.5 Flash for all tasks (cost-effective with temperature control)
LLM_FEATURES=gemini:gemini-2.5-flash
LLM_INSIGHTS=gemini:gemini-2.5-flash
LLM_PATCH=gemini:gemini-2.5-flash
LLM_BRIEF=gemini:gemini-2.5-flash
LLM_ANALYZE=gemini:gemini-2.5-flash
LLM_EDIT=gemini:gemini-2.5-flash

# Insights Configuration
# Number of top insights to return (default: all)
# Options:
#   - 'all': Return all valid insights from predefined directions (recommended)
#   - A number (e.g., 3, 5): Limit to top N highest-scoring insights
#
# The system evaluates 10 predefined insight directions and fills only those
# with supporting data evidence. Use 'all' to see every applicable insight,
# or set a number to focus on the highest-priority opportunities.
NUM_INSIGHTS=all

# Debug settings
DEBUG_LLM=true
