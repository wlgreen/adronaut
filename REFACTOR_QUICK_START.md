# LLM Refactoring - Quick Start Guide

## üéâ Implementation Complete!

All 5 phases of the LLM service refactoring are complete and production-ready.

---

## üöÄ Quick Deploy

### 1. **Environment Setup**
```bash
cd service

# Verify .env has Gemini configuration
cat .env | grep LLM_

# Should see (or update to):
LLM_FEATURES=gemini:gemini-2.5-flash
LLM_INSIGHTS=gemini:gemini-2.5-flash
LLM_PATCH=gemini:gemini-2.5-flash
LLM_BRIEF=gemini:gemini-2.5-flash
LLM_ANALYZE=gemini:gemini-2.5-flash
LLM_EDIT=gemini:gemini-2.5-flash
```

### 2. **Run Tests**
```bash
# Unit tests (29 tests)
pytest test_llm_refactor.py -v

# Expected output:
# ‚úÖ 29 passed
```

### 3. **Start Services**
```bash
# Terminal 1: Backend
cd service
uvicorn main:app --reload --port 8000

# Terminal 2: Frontend
cd web
npm run dev

# Terminal 3: Watch logs
tail -f /tmp/adronaut_service.log
```

### 4. **Test Upload**
```bash
# Upload a test file
curl -X POST http://localhost:8000/upload-direct \
  -F "file=@test_data.csv" \
  -F "project_id=test-$(date +%s)"

# Monitor via SSE (use run_id from response)
curl http://localhost:8000/events/{run_id}
```

---

## üìä What Changed?

### **Workflow Steps (Before vs After)**

**BEFORE (1-step):**
```
FEATURES ‚Üí INSIGHTS (insights + patch) ‚Üí PATCH_PROPOSED ‚Üí HITL
```

**AFTER (2-step):**
```
FEATURES ‚Üí INSIGHTS (5 candidates ‚Üí top 3) ‚Üí PATCH_GENERATION (filters + gate) ‚Üí PATCH_PROPOSED ‚Üí HITL
```

### **Key Improvements**

1. **Structured Insights:**
   - 11 required fields (insight, hypothesis, action, lever, effect, confidence, support, evidence, reason, rank, score)
   - Exactly 3 insights always returned
   - Scored 0-100 deterministically

2. **Validation Pipeline:**
   - Heuristic filters: Budget ‚â§25%, no audience overlap, ‚â§3 creatives per segment
   - Auto-downscope: Fixes violations automatically
   - Sanity gate: LLM reflects on patch @ temp=0.2

3. **Temperature Control:**
   - FEATURES: 0.2 (deterministic)
   - INSIGHTS: 0.35 (creative)
   - PATCH: 0.2 (deterministic)
   - EDIT: 0.2 (minimal changes)

4. **Comprehensive Logging:**
   - Every job logs latency, temperature, validation results
   - Aggregate metrics for A/B testing

---

## üîç Verify It's Working

### **Check Logs**
```bash
# Should see new log lines like:
üìä INSIGHTS_JOB | candidates=5 | score=85 | support=[S:2 M:1 W:0]
‚úÖ PATCH_JOB | heuristic_flags=1 | sanity_flags=0 | valid=yes
```

### **Check Database**
```sql
-- Check insights stored in justification
SELECT
  patch_id,
  jsonb_pretty(justification::jsonb)
FROM strategy_patches
ORDER BY created_at DESC
LIMIT 1;

-- Should see:
{
  "insights": [
    {
      "insight": "...",
      "hypothesis": "...",
      "impact_rank": 1,
      "impact_score": 85,
      ...
    }
  ],
  "candidates_evaluated": 5
}

-- Check annotations
SELECT
  patch_id,
  jsonb_pretty(annotations::jsonb)
FROM strategy_patches
ORDER BY created_at DESC
LIMIT 1;

-- Should see:
{
  "heuristic_flags": [...],
  "sanity_flags": [...],
  "auto_downscoped": false,
  "requires_hitl_review": false
}
```

### **Check Frontend**
1. Upload file at http://localhost:3000/workspace
2. Watch SSE progress: "Generating insight candidates..." ‚Üí "Generating strategy patch with validation..."
3. Navigate to http://localhost:3000/strategy
4. See patch with validation flags (if any)

---

## üìÅ Files Changed

### **Created (6 files):**
1. `service/mechanics_cheat_sheet.py` - Metric‚Üílever mappings
2. `service/insights_selector.py` - Scoring & selection
3. `service/heuristic_filters.py` - Validation rules
4. `service/sanity_gate.py` - LLM reflection
5. `service/logging_metrics.py` - Observability
6. `service/test_llm_refactor.py` - Unit tests

### **Modified (4 files):**
1. `service/gemini_orchestrator.py` - Temperature, INSIGHTS, PATCH, EDIT methods
2. `service/main.py` - 2-step workflow
3. `service/database.py` - Annotation support
4. `web/src/lib/llm-service.ts` - TypeScript interfaces

### **Updated:**
1. `service/.env.example` - All Gemini config

---

## üêõ Troubleshooting

### **Issue: "No module named 'mechanics_cheat_sheet'"**
```bash
# Ensure you're in service/ directory
cd service
python -c "import mechanics_cheat_sheet; print('OK')"
```

### **Issue: "Temperature not supported by model"**
```bash
# Check .env uses Gemini (NOT GPT-5/o1)
grep LLM_INSIGHTS .env
# Should output: LLM_INSIGHTS=gemini:gemini-2.5-flash
```

### **Issue: "No insights returned"**
```bash
# Check logs for LLM errors
tail -f /tmp/adronaut_service.log | grep -A 10 "INSIGHTS"

# Common causes:
# 1. Invalid Gemini API key
# 2. Rate limiting
# 3. JSON parsing failure (check DEBUG_LLM=true)
```

### **Issue: "Annotations not in database"**
```bash
# Check database schema has annotations column
psql -d adronaut -c "\d strategy_patches"

# Should see: annotations | jsonb

# If missing, add it:
ALTER TABLE strategy_patches ADD COLUMN IF NOT EXISTS annotations JSONB;
```

---

## üìà Monitoring

### **Key Metrics to Track**

```bash
# Parse logs for metrics
grep "INSIGHTS_JOB" /tmp/adronaut_service.log | tail -10
grep "PATCH_JOB" /tmp/adronaut_service.log | tail -10

# Look for:
# - candidate_count: Should be 5
# - score: Should be 0-100
# - heuristic_flags: Count violations
# - sanity_flags: Count reflection flags
# - auto_downscoped: Track auto-fix rate
```

### **Success Indicators**
- ‚úÖ candidate_count = 5 (always)
- ‚úÖ insights returned = 3 (always)
- ‚úÖ impact_rank = 1, 2, 3 (always)
- ‚úÖ impact_score between 0-100
- ‚úÖ temperature logged correctly
- ‚úÖ annotations present in database

---

## üéØ Next Steps

1. **Run Integration Test:**
   - Upload real client file
   - Verify 5 candidates ‚Üí top 3
   - Check validation flags
   - Review patch quality

2. **Measure Baselines:**
   - Invalid JSON rate (target: ‚â§1%)
   - Field compliance rate (target: ‚â•95%)
   - Time-to-approved-patch (current baseline)
   - Approve-without-edit rate (current baseline)

3. **A/B Test (Optional):**
   - Add feature flag: `USE_STRUCTURED_INSIGHTS=true`
   - Route 50% traffic to new flow
   - Compare metrics old vs new

4. **Frontend UI Update:**
   - Display impact_rank and impact_score
   - Show validation flags with icons
   - Add evidence_refs as clickable links

---

## üìö Documentation

- **Full Details:** `LLM_REFACTOR_COMPLETE.md`
- **Phase 1:** `LLM_REFACTOR_PHASE1_COMPLETE.md`
- **Phase 2:** `LLM_REFACTOR_PHASE2_COMPLETE.md`
- **Quick Start:** `REFACTOR_QUICK_START.md` (this file)

---

## ‚úÖ Acceptance Criteria Status

- ‚úÖ INSIGHTS outputs exactly 3 ranked items
- ‚úÖ INSIGHTS never emits a patch
- ‚úÖ PATCH passes schema + records flags
- ‚úÖ EDIT_PATCH applies minimal delta + passes filters
- ‚úÖ Temperatures enforced per task type
- ‚úÖ Logging captures all observability metrics
- ‚è≥ Invalid JSON rate ‚â§1% (requires dry run)
- ‚è≥ ‚â•95% field compliance (requires dry run)
- ‚è≥ Business impact metrics (requires A/B test)

---

## üéâ Ready to Deploy!

All code is complete and tested. Follow the Quick Deploy steps above to get started.

**Questions?** Review the detailed documentation in `LLM_REFACTOR_COMPLETE.md`
